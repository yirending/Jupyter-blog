{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) Part 2\n",
    "\n",
    "In this post, we will discuss PCA in a theoretical perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We state and prove several theorems that will be useful for us later.\n",
    "$\\newcommand{\\bs}{\\boldsymbol}$\n",
    "$\\newcommand{\\tr}{^{\\top}}$\n",
    ">**Theorem 1**. Let $\\bs{A} \\in \\mathbb{R}^{n\\times n}$ be a real valued symmetric matrix, and $\\bs{x}$ be a real vector in $\\mathbb{R}^n$.  Then the quadratic expression $f(\\bs{x}) = \\bs{x}\\tr \\bs{A} \\bs{x}$ subject to $||\\bs{x}||_2=1$ takes on maximum (minimum) values at its unit eigenvector $\\bs{x}$ corresponding to its maximum (minimum) eigenvalue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an visualization of the constraint optimization. Basically, we are optimizing a quadratic function over a sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"quadratic_optimize.png\" alt=\"drawing\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Proof_. Since $\\bs{A}$ is symmetric, then by the `spectral theorem`, it is orthogonally diagonalizable.  This means that there exists orthogonal matrix $\\bs{Q}$ and diagonal matrix $\\bs{\\Lambda}$ such that\n",
    "\n",
    "$$\\bs{A} = \\bs{Q} \\bs{\\Lambda} \\bs{Q}\\tr.$$\n",
    "\n",
    "Let $\\bs{\\Lambda} = \\text{diag}(\\bs{\\lambda})$, where $\\lambda_i \\in \\bs{\\lambda}$ is the eigenvalue corresponding to the $i$th column of the matrix $\\bs{Q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\bs{x}_i$ be an unit eigenvector of $\\bs{A}$ corresponding to the eigenvector $\\lambda_i$.  Then \n",
    "\n",
    "$$f(\\bs{x}_i) = \\bs{x}_i\\tr\\bs{A}\\bs{x}_i = \\bs{x}_i\\tr\\lambda_i\\bs{x}_i = \\lambda_i\\bs{x}_i\\tr\\bs{x}_i = \\lambda_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\bs{Q} = [\\bs{q}_1,...,\\bs{q}_n]$ where $\\bs{q}_i \\in \\mathbb{R}^n$ and the set of $\\bs{q}_i$'s are orthogonal unit vectors; hence form an orthonormal basis of $\\mathbb{R}^n$. This means that for all unit vector $\\bs{x} \\in \\mathbb{R}^n$, $\\bs{x}$ can be expressed as a linear combination of the orthonormal set $\\{\\bs{q}_1,...,\\bs{q}_n\\}$,\n",
    "\n",
    "$$\n",
    "\\bs{x} = c_1 \\bs{q}_1 + c_2\\bs{q}_2 + \\cdots + c_n\\bs{q}_n.\n",
    "$$\n",
    "\n",
    "Furthermore, since $||x||_2=1$, we have that $c_1^1+x_2^2+\\cdots + c_n^2=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\lambda_{\\text{max}}$ and $\\lambda_{\\text{min}}$ be the maximum and minimum of all eigenvalues $\\{\\lambda_1,...,\\lambda_n\\}$. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\bs{x}) & = (c_1\\bs{q}_1 + \\cdots + c_n\\bs{q}_n)\\tr \\bs{A}(c_1\\bs{q}_1 + \\cdots + c_n\\bs{q}_n) \\\\\n",
    "& = c_1^2 \\bs{q}_1\\tr\\bs{A}\\bs{q}_1 + c_1 c_2 \\bs{q}_1\\tr \\bs{A}\\bs{q}_2 + \\cdots c_n^2 \\bs{q}_n\\tr \\bs{A}\\bs{q}_n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\bs{q}_i\\tr \\bs{q}_j = 0$ whenever $i\\neq j$, the above expression simplifies to \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\bs{x}) &= c_1^2 \\bs{q}_1\\tr \\bs{A}\\bs{q}_1 + c_2^2 \\bs{q}_2 \\bs{A}\\bs{q}_2 + \\cdots c_n^2 \\bs{q}_n\\tr \\bs{A}\\bs{q}_n \\\\\n",
    "&= c_1^2 \\lambda_1 + c_2^2 \\lambda_2 + \\cdots + c_n^2 \\lambda_n \\\\\n",
    "&\\leq \\lambda_{\\text{max}}(c_1^2+c_2^2 + \\cdots + c_n^2) \\\\\n",
    "& = \\lambda_{\\text{max}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can show that $f(\\bs{x}) \\geq \\lambda_{\\text{min}}$. This proves the theorem. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we introduce the concepts of `Frobenius norm` and `trace`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 1**. The **Frobenius norm** of a matrix $A$ is defined to be\n",
    "$$\n",
    "||A||_F = \\sqrt{\\sum_{i,j}A_{i,j}^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 2**. The **trace operator** of a matrix $A$ is defined to be\n",
    "$$\n",
    "\\text{Tr}(\\bs{A}) = \\sum_i \\bs{A}_{i,i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to the following theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Theorem 2**. The following properties hold for a matrix $A$:\n",
    "1. $\\displaystyle ||A||_F = \\sqrt{\\text{Tr}(\\bs{A}\\bs{A}\\tr)}$\n",
    "2. $\\text{Tr}(\\bs{A}) = \\text{Tr}(\\bs{A}\\tr)$\n",
    "3. If $\\bs{A}\\in \\mathbb{R}^{m\\times n}$ and $\\bs{B}\\in \\mathbb{R}^{n\\times m}$, then \n",
    "$$\n",
    "\\text{Tr}(\\bs{A}\\bs{B}) = \\text{Tr}(\\bs{B}\\bs{A})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Proof of 1_. This follows easiliy from the definition of matrix multiplication,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Tr}(\\bs{A}\\bs{A}\\tr) & = \\sum_i \\bs{A}_{1,i}^2 + \\sum_i \\bs{A}_{2,i}^2 + \\cdots + \\sum_i \\bs{A}_{n,i}^2 \\\\\n",
    "&= \\sum_{i,j} \\bs{A}_{i,j}^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Proof of 2_. Obviously the diagonal elements are invariant to transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Proof of 3_. Again we use the definition of matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Tr}(\\bs{A}\\bs{B}) &= \\sum_i \\bs{A}_{1,i}\\bs{B}_{i,1} + \\sum_i \\bs{A}_{2,i}\\bs{B}_{i,2} + \\cdots + \\sum_i \\bs{A}_{m,i}\\bs{B}_{i,m} \\\\\n",
    "&= \\sum_{i,j} \\bs{A}_{j,i} \\bs{B}_{i,j} \\\\\n",
    "&= \\sum_{i,j} \\bs{B}_{j,i}\\bs{A}_{i,j} \\\\\n",
    "&= \\text{Tr}(\\bs{B}\\bs{A})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**. Theorem 2 (3) can be generalized.  If the shape of the corresponding matrices allow multiplication, then trace is invariant to cyclic permutation of the order:\n",
    "$$\n",
    "\\text{Tr}\\left(\\prod_{i=1}^n \\bs{A}^{(i)}\\right) = \\text{Tr}\\left(\\bs{A}^{(n)}\\prod_{i=1}^{n-1} \\bs{A}^{(i)}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
